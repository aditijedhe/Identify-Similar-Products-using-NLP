{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "342ce37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from wordcloud import WordCloud \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.pipeline import Pipeline\n",
    "import spacy\n",
    "from collections import Counter\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9ec438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dataset\n",
    "amazon = pd.read_csv(\"E:/A/P/internships/shack lab/DS - Assignment Part 2 data set/amz_com-ecommerce_sample.csv\", encoding=\"unicode_escape\")\n",
    "flipkart = pd.read_csv(\"E:/A/P/internships/shack lab/DS - Assignment Part 2 data set/flipkart_com-ecommerce_sample.csv\", encoding=\"unicode_escape\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804a3ca4",
   "metadata": {},
   "source": [
    "### Data Processing and Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1684e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting necessary features in a new dataframe.\n",
    "\n",
    "amz = amazon[['product_name','retail_price','discounted_price']]\n",
    "flk = flipkart[['product_name','retail_price','discounted_price']]\n",
    "\n",
    "# concating the title and description of the product as 'text'\n",
    "amz[\"text\"]=amazon['product_name']+amazon['description']\n",
    "\n",
    "#dropping missing values present in text field\n",
    "amz = amz.dropna(subset=['text'])\n",
    "\n",
    "#resetting the indices\n",
    "amz.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# performing teh same procedure for flipkart datframe as well\n",
    "flk[\"text\"]=flipkart['product_name']+flipkart['description']\n",
    "flk = flk.dropna(subset=['text'])\n",
    "flk.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e61e9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw=nltk.corpus.stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acb75a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctation marks from the text\n",
    "def remove_punc(data):\n",
    "    pattern = r'[' + string.punctuation + ']'\n",
    "    data['text']=data['text'].map(lambda m:re.sub(pattern,\" \",str(m)))\n",
    "    return data\n",
    "\n",
    "# converting all characters into lower case\n",
    "def lower(data):\n",
    "    data['text1']=copdataydata['text'].map(lambda m:m.lower())\n",
    "    return data\n",
    "\n",
    "# tokenizing the text\n",
    "def tokenization(text):\n",
    "    tokens = re.split(' ',text)\n",
    "    return tokens\n",
    "\n",
    "def token(data):\n",
    "    data['text']= data['text'].apply(lambda x: tokenization(x))\n",
    "    return data\n",
    "\n",
    "# removing stopwords\n",
    "def remove_SW(data):\n",
    "    data['text']=data['text'].apply(lambda x: [item for item in x if item not in sw])\n",
    "    return data\n",
    "\n",
    "# removing digits \n",
    "def remove_digits(data):\n",
    "    data['text']=data['text'].apply(lambda x: [item for item in x if not item.isdigit()])\n",
    "    return data\n",
    "\n",
    "#lemmatization\n",
    "def lemmatize(data):\n",
    "    data['text']=data['text'].apply(lambda x: [lemmatizer.lemmatize(item) for item in x])\n",
    "    return data\n",
    "\n",
    "# removing empty tokens\n",
    "def remove_empty_tokens(data):\n",
    "    data['text']=data['text'].apply(lambda x: [item for item in x if item !=''])\n",
    "    return data\n",
    "\n",
    "# removing single characters\n",
    "def remove_single_letters(data):\n",
    "    data['text']=data['text'].apply(lambda x: [item for item in x if len(item) > 1])\n",
    "    return data\n",
    "\n",
    "# detokinizing\n",
    "def detoken(data):\n",
    "    data['text']= data['text'].apply(lambda x: TreebankWordDetokenizer().detokenize(x))\n",
    "    return data\n",
    "\n",
    "#replacing empty spaces \n",
    "def replace_spaces(x,space,second):\n",
    "    result = x.replace(space, second)\n",
    "    return result\n",
    "\n",
    "#removing spaces\n",
    "def remove_space(data):\n",
    "    data['text']= data['text'].apply(lambda x: replace_spaces(x,'  ',' '))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c788d97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline function to process the data.\n",
    "new_amz = amz.pipe(remove_punc).pipe(token).pipe(remove_SW).pipe(remove_digits).pipe(lemmatize).pipe(remove_empty_tokens).pipe(remove_single_letters).pipe(detoken).pipe(remove_space)\n",
    "new_flk = flk.pipe(remove_punc).pipe(token).pipe(remove_SW).pipe(remove_digits).pipe(lemmatize).pipe(remove_empty_tokens).pipe(remove_single_letters).pipe(detoken).pipe(remove_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ebb1b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>retail_price</th>\n",
       "      <th>discounted_price</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alisha Solid Women's Cycling Shorts</td>\n",
       "      <td>982</td>\n",
       "      <td>438</td>\n",
       "      <td>Alisha Solid Women Cycling ShortsKey Features ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FabHomeDecor Fabric Double Sofa Bed</td>\n",
       "      <td>32143</td>\n",
       "      <td>29121</td>\n",
       "      <td>FabHomeDecor Fabric Double Sofa BedFabHomeDeco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AW Bellies</td>\n",
       "      <td>991</td>\n",
       "      <td>551</td>\n",
       "      <td>AW BelliesKey Features AW Bellies Sandals Wedg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alisha Solid Women's Cycling Shorts</td>\n",
       "      <td>694</td>\n",
       "      <td>325</td>\n",
       "      <td>Alisha Solid Women Cycling ShortsKey Features ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sicons All Purpose Arnica Dog Shampoo</td>\n",
       "      <td>208</td>\n",
       "      <td>258</td>\n",
       "      <td>Sicons All Purpose Arnica Dog ShampooSpecifica...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            product_name  retail_price  discounted_price  \\\n",
       "0    Alisha Solid Women's Cycling Shorts           982               438   \n",
       "1    FabHomeDecor Fabric Double Sofa Bed         32143             29121   \n",
       "2                             AW Bellies           991               551   \n",
       "3    Alisha Solid Women's Cycling Shorts           694               325   \n",
       "4  Sicons All Purpose Arnica Dog Shampoo           208               258   \n",
       "\n",
       "                                                text  \n",
       "0  Alisha Solid Women Cycling ShortsKey Features ...  \n",
       "1  FabHomeDecor Fabric Double Sofa BedFabHomeDeco...  \n",
       "2  AW BelliesKey Features AW Bellies Sandals Wedg...  \n",
       "3  Alisha Solid Women Cycling ShortsKey Features ...  \n",
       "4  Sicons All Purpose Arnica Dog ShampooSpecifica...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_amz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ee4904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporting the cleaned dataframe\n",
    "new_amz.to_csv(\"E:/A/P/internships/shack lab/DS - Assignment Part 2 data set/cleaned_amz.csv\")\n",
    "new_flk.to_csv(\"E:/A/P/internships/shack lab/DS - Assignment Part 2 data set/cleaned_flk.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b704631",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the cleaned Amazon dataset: (19998,)\n",
      "Dimension of the cleaned Flipkart dataset: (19998,)\n"
     ]
    }
   ],
   "source": [
    "# dimenaion of the cleaned dataframe\n",
    "print(\"Dimension of the cleaned Amazon dataset: \" + str(new_amz['text'].shape))\n",
    "print(\"Dimension of the cleaned Flipkart dataset: \" + str(new_flk['text'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cd29482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizing the text using Tf-Idf Method\n",
    "\n",
    "amz_X = new_amz['text']\n",
    "flk_X = new_flk['text']\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "tfidf_amz = tfidf.fit_transform(amz_X)\n",
    "tfidf_flk = tfidf.transform(flk_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7e0260b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>0008m</th>\n",
       "      <th>000hrs</th>\n",
       "      <th>001</th>\n",
       "      <th>0010m</th>\n",
       "      <th>0011m</th>\n",
       "      <th>0018m</th>\n",
       "      <th>001flipkart</th>\n",
       "      <th>001pink</th>\n",
       "      <th>003</th>\n",
       "      <th>...</th>\n",
       "      <th>½to</th>\n",
       "      <th>½with</th>\n",
       "      <th>½ï</th>\n",
       "      <th>âº</th>\n",
       "      <th>âºc</th>\n",
       "      <th>â¼</th>\n",
       "      <th>â½</th>\n",
       "      <th>â¾</th>\n",
       "      <th>ãº</th>\n",
       "      <th>å¾ã</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27777 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000 0008m 000hrs  001 0010m 0011m 0018m 001flipkart 001pink  003  ...  ½to  \\\n",
       "0  0.0   0.0    0.0  0.0   0.0   0.0   0.0         0.0     0.0  0.0  ...  0.0   \n",
       "1  0.0   0.0    0.0  0.0   0.0   0.0   0.0         0.0     0.0  0.0  ...  0.0   \n",
       "2  0.0   0.0    0.0  0.0   0.0   0.0   0.0         0.0     0.0  0.0  ...  0.0   \n",
       "3  0.0   0.0    0.0  0.0   0.0   0.0   0.0         0.0     0.0  0.0  ...  0.0   \n",
       "4  0.0   0.0    0.0  0.0   0.0   0.0   0.0         0.0     0.0  0.0  ...  0.0   \n",
       "\n",
       "  ½with   ½ï   âº  âºc   â¼   â½   â¾   ãº  å¾ã  \n",
       "0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 27777 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#viewing the vectorized text as dataframe\n",
    "pd.DataFrame(tfidf_flk.toarray(), columns= [tfidf.get_feature_names()]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bbbd07d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>0008m</th>\n",
       "      <th>000hrs</th>\n",
       "      <th>001</th>\n",
       "      <th>0010m</th>\n",
       "      <th>0011m</th>\n",
       "      <th>0018m</th>\n",
       "      <th>001flipkart</th>\n",
       "      <th>001pink</th>\n",
       "      <th>003</th>\n",
       "      <th>...</th>\n",
       "      <th>½to</th>\n",
       "      <th>½with</th>\n",
       "      <th>½ï</th>\n",
       "      <th>âº</th>\n",
       "      <th>âºc</th>\n",
       "      <th>â¼</th>\n",
       "      <th>â½</th>\n",
       "      <th>â¾</th>\n",
       "      <th>ãº</th>\n",
       "      <th>å¾ã</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27777 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000 0008m 000hrs  001 0010m 0011m 0018m 001flipkart 001pink  003  ...  ½to  \\\n",
       "0  0.0   0.0    0.0  0.0   0.0   0.0   0.0         0.0     0.0  0.0  ...  0.0   \n",
       "1  0.0   0.0    0.0  0.0   0.0   0.0   0.0         0.0     0.0  0.0  ...  0.0   \n",
       "2  0.0   0.0    0.0  0.0   0.0   0.0   0.0         0.0     0.0  0.0  ...  0.0   \n",
       "3  0.0   0.0    0.0  0.0   0.0   0.0   0.0         0.0     0.0  0.0  ...  0.0   \n",
       "4  0.0   0.0    0.0  0.0   0.0   0.0   0.0         0.0     0.0  0.0  ...  0.0   \n",
       "\n",
       "  ½with   ½ï   âº  âºc   â¼   â½   â¾   ãº  å¾ã  \n",
       "0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 27777 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tfidf_amz.toarray(), columns= [tfidf.get_feature_names()]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ab9225",
   "metadata": {},
   "source": [
    "\n",
    "#### I am taking a sample of 1000 from the dataframe, because it is computationally expensive to check each and every entry within themselves when the size of the dataset is large i.e., 2000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2525764e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#getting the indices of the text with similarity score more than 0.95.\n",
    "# the indices are saved in a list format as [i,j] where i is the index of the amazon product and j is of flipkart product\n",
    "\n",
    "index=[]\n",
    "for i in range(1001):\n",
    "    for j in range(1001):\n",
    "        matches = cosine_similarity(tfidf_amz[i], tfidf_flk[j])\n",
    "        if matches == 1 or matches >0.95:\n",
    "            index.append([i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2738b126",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0],\n",
       " [0, 6],\n",
       " [0, 13],\n",
       " [0, 15],\n",
       " [1, 1],\n",
       " [1, 7],\n",
       " [1, 16],\n",
       " [1, 19],\n",
       " [2, 2],\n",
       " [3, 3]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#viewing the first 10 elements of the list \"index\"\n",
    "index[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ae33c57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2635"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "652b864c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# saving the product name, retail price and discount price of similar products in a dataframe.\n",
    "result=pd.DataFrame()\n",
    "\n",
    "for i,j in index:\n",
    "    a=pd.concat([new_amz[['product_name', \"retail_price\", \"discounted_price\"]].iloc[[i]],new_flk[['product_name', \"retail_price\", \"discounted_price\"]].iloc[[i]]], axis=1)\n",
    "    result = pd.concat([result,a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85b29b5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>retail_price</th>\n",
       "      <th>discounted_price</th>\n",
       "      <th>product_name</th>\n",
       "      <th>retail_price</th>\n",
       "      <th>discounted_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alisha Solid Women's Cycling Shorts</td>\n",
       "      <td>982</td>\n",
       "      <td>438</td>\n",
       "      <td>Alisha Solid Women's Cycling Shorts</td>\n",
       "      <td>999.0</td>\n",
       "      <td>379.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alisha Solid Women's Cycling Shorts</td>\n",
       "      <td>982</td>\n",
       "      <td>438</td>\n",
       "      <td>Alisha Solid Women's Cycling Shorts</td>\n",
       "      <td>999.0</td>\n",
       "      <td>379.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alisha Solid Women's Cycling Shorts</td>\n",
       "      <td>982</td>\n",
       "      <td>438</td>\n",
       "      <td>Alisha Solid Women's Cycling Shorts</td>\n",
       "      <td>999.0</td>\n",
       "      <td>379.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alisha Solid Women's Cycling Shorts</td>\n",
       "      <td>982</td>\n",
       "      <td>438</td>\n",
       "      <td>Alisha Solid Women's Cycling Shorts</td>\n",
       "      <td>999.0</td>\n",
       "      <td>379.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FabHomeDecor Fabric Double Sofa Bed</td>\n",
       "      <td>32143</td>\n",
       "      <td>29121</td>\n",
       "      <td>FabHomeDecor Fabric Double Sofa Bed</td>\n",
       "      <td>32157.0</td>\n",
       "      <td>22646.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          product_name  retail_price  discounted_price  \\\n",
       "0  Alisha Solid Women's Cycling Shorts           982               438   \n",
       "0  Alisha Solid Women's Cycling Shorts           982               438   \n",
       "0  Alisha Solid Women's Cycling Shorts           982               438   \n",
       "0  Alisha Solid Women's Cycling Shorts           982               438   \n",
       "1  FabHomeDecor Fabric Double Sofa Bed         32143             29121   \n",
       "\n",
       "                          product_name  retail_price  discounted_price  \n",
       "0  Alisha Solid Women's Cycling Shorts         999.0             379.0  \n",
       "0  Alisha Solid Women's Cycling Shorts         999.0             379.0  \n",
       "0  Alisha Solid Women's Cycling Shorts         999.0             379.0  \n",
       "0  Alisha Solid Women's Cycling Shorts         999.0             379.0  \n",
       "1  FabHomeDecor Fabric Double Sofa Bed       32157.0           22646.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6725ea83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2635, 6)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
